//===- FgpuInstructionSelector.cpp ------------------------------*- C++ -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
/// \file
/// This file implements the targeting of the InstructionSelector class for
/// Fgpu.
/// \todo This should be generated by TableGen.
//===----------------------------------------------------------------------===//

#include "MCTargetDesc/FgpuInstPrinter.h"
#include "FgpuMachineFunction.h"
#include "FgpuRegisterBankInfo.h"
#include "FgpuTargetMachine.h"
#include "llvm/CodeGen/GlobalISel/InstructionSelectorImpl.h"
#include "llvm/CodeGen/GlobalISel/MachineIRBuilder.h"
#include "llvm/CodeGen/MachineJumpTableInfo.h"
#include "llvm/IR/IntrinsicsFgpu.h"

#define DEBUG_TYPE "fgpu-isel"

using namespace llvm;

namespace {

#define GET_GLOBALISEL_PREDICATE_BITSET
#include "FgpuGenGlobalISel.inc"
#undef GET_GLOBALISEL_PREDICATE_BITSET

class FgpuInstructionSelector : public InstructionSelector {
public:
  FgpuInstructionSelector(const FgpuTargetMachine &TM, const FgpuSubtarget &STI,
                          const FgpuRegisterBankInfo &RBI);

  bool select(MachineInstr &I) override;
  static const char *getName() { return DEBUG_TYPE; }

private:
  bool selectImpl(MachineInstr &I, CodeGenCoverage &CoverageInfo) const;
  bool isRegInGprb(Register Reg, MachineRegisterInfo &MRI) const;
  bool isRegInFprb(Register Reg, MachineRegisterInfo &MRI) const;
  bool materialize32BitImm(Register DestReg, APInt Imm,
                           MachineIRBuilder &B) const;
  bool selectCopy(MachineInstr &I, MachineRegisterInfo &MRI) const;
  const TargetRegisterClass *
  getRegClassForTypeOnBank(Register Reg, MachineRegisterInfo &MRI) const;
  unsigned selectLoadStoreOpCode(MachineInstr &I,
                                 MachineRegisterInfo &MRI) const;
  bool buildUnalignedStore(MachineInstr &I, unsigned Opc,
                           MachineOperand &BaseAddr, unsigned Offset,
                           MachineMemOperand *MMO) const;
  bool buildUnalignedLoad(MachineInstr &I, unsigned Opc, Register Dest,
                          MachineOperand &BaseAddr, unsigned Offset,
                          Register TiedDest, MachineMemOperand *MMO) const;

  const FgpuTargetMachine &TM;
  const FgpuSubtarget &STI;
  const FgpuInstrInfo &TII;
  const FgpuRegisterInfo &TRI;
  const FgpuRegisterBankInfo &RBI;

#define GET_GLOBALISEL_PREDICATES_DECL
#include "FgpuGenGlobalISel.inc"
#undef GET_GLOBALISEL_PREDICATES_DECL

#define GET_GLOBALISEL_TEMPORARIES_DECL
#include "FgpuGenGlobalISel.inc"
#undef GET_GLOBALISEL_TEMPORARIES_DECL
};

} // end anonymous namespace

#define GET_GLOBALISEL_IMPL
#include "FgpuGenGlobalISel.inc"
#undef GET_GLOBALISEL_IMPL

FgpuInstructionSelector::FgpuInstructionSelector(
    const FgpuTargetMachine &TM, const FgpuSubtarget &STI,
    const FgpuRegisterBankInfo &RBI)
    : InstructionSelector(), TM(TM), STI(STI), TII(*STI.getInstrInfo()),
      TRI(*STI.getRegisterInfo()), RBI(RBI),

#define GET_GLOBALISEL_PREDICATES_INIT
#include "FgpuGenGlobalISel.inc"
#undef GET_GLOBALISEL_PREDICATES_INIT
#define GET_GLOBALISEL_TEMPORARIES_INIT
#include "FgpuGenGlobalISel.inc"
#undef GET_GLOBALISEL_TEMPORARIES_INIT
{
}

bool FgpuInstructionSelector::isRegInGprb(Register Reg,
                                          MachineRegisterInfo &MRI) const {
  return RBI.getRegBank(Reg, MRI, TRI)->getID() == Fgpu::GPRBRegBankID;
}

bool FgpuInstructionSelector::isRegInFprb(Register Reg,
                                          MachineRegisterInfo &MRI) const {
  return RBI.getRegBank(Reg, MRI, TRI)->getID() == Fgpu::VFPRegBankID;
}

bool FgpuInstructionSelector::selectCopy(MachineInstr &I,
                                         MachineRegisterInfo &MRI) const {
  Register DstReg = I.getOperand(0).getReg();
  if (Register::isPhysicalRegister(DstReg))
    return true;

  const TargetRegisterClass *RC = getRegClassForTypeOnBank(DstReg, MRI);
  if (!RBI.constrainGenericRegister(DstReg, *RC, MRI)) {
    LLVM_DEBUG(dbgs() << "Failed to constrain " << TII.getName(I.getOpcode())
                      << " operand\n");
    return false;
  }
  return true;
}

const TargetRegisterClass *FgpuInstructionSelector::getRegClassForTypeOnBank(
    Register Reg, MachineRegisterInfo &MRI) const {
  const LLT Ty = MRI.getType(Reg);
  const unsigned TySize = Ty.getSizeInBits();

  if (isRegInGprb(Reg, MRI)) {
    assert((Ty.isScalar() || Ty.isPointer()) && TySize == 32 &&
           "Register class not available for LLT, register bank combination");
    return &Fgpu::GPROutRegClass;
  }

  if (isRegInFprb(Reg, MRI)) {
    return &Fgpu::VecRegsRegClass;
  }

  llvm_unreachable("Unsupported register bank.");
}

bool FgpuInstructionSelector::materialize32BitImm(Register DestReg, APInt Imm,
                                                  MachineIRBuilder &B) const {
  assert(Imm.getBitWidth() == 32 && "Unsupported immediate size.");
  // Ori zero extends immediate. Used for values with zeros in high 16 bits.
  if (Imm.getHiBits(18).isNullValue()) {
    MachineInstr *Inst =
        B.buildInstr(Fgpu::ORi, {DestReg}, {Register(Fgpu::ZERO)})
            .addImm(Imm.getLoBits(14).getLimitedValue());
    return constrainSelectedInstRegOperands(*Inst, TII, TRI, RBI);
  }
  //TODO: re-enable once understand the behaviour of Li/LUi better
//  if (Imm.getHiBits(16).isNullValue()) {
//    MachineInstr *Inst =
//        B.buildInstr(Fgpu::Li, {DestReg}, {Register(Fgpu::ZERO)})
//            .addImm(Imm.getLoBits(16).getLimitedValue());
//    return constrainSelectedInstRegOperands(*Inst, TII, TRI, RBI);
//  }
//  // Lui places immediate in high 16 bits and sets low 16 bits to zero.
//  if (Imm.getLoBits(16).isNullValue()) {
//    MachineInstr *Inst = B.buildInstr(Fgpu::LUi, {DestReg}, {})
//                             .addImm(Imm.getHiBits(16).getLimitedValue());
//    return constrainSelectedInstRegOperands(*Inst, TII, TRI, RBI);
//  }
  // Values that cannot be materialized with single immediate instruction.
  // Register LUiReg = B.getMRI()->createVirtualRegister(&Fgpu::GPROutRegClass);
  MachineInstr *LUi = B.buildInstr(Fgpu::LUi, {DestReg}, {})
                          .addImm(Imm.getHiBits(16).getLimitedValue());
  MachineInstr *Li = B.buildInstr(Fgpu::Li, {DestReg}, {})
                          .addImm(Imm.getLoBits(16).getLimitedValue());
  if (!constrainSelectedInstRegOperands(*LUi, TII, TRI, RBI))
    return false;
  if (!constrainSelectedInstRegOperands(*Li, TII, TRI, RBI))
    return false;
  return true;
}

/// When I.getOpcode() is returned, we failed to select FGPU instruction opcode.
unsigned
FgpuInstructionSelector::selectLoadStoreOpCode(MachineInstr &I,
                                               MachineRegisterInfo &MRI) const {
  const Register ValueReg = I.getOperand(0).getReg();
  const LLT Ty = MRI.getType(ValueReg);
  const unsigned TySize = Ty.getSizeInBits();
  const unsigned MemSizeInBytes = (*I.memoperands_begin())->getSize();
  unsigned Opc = I.getOpcode();
  const bool isStore = Opc == TargetOpcode::G_STORE;

  if (isRegInGprb(ValueReg, MRI)) {
    assert(((Ty.isScalar() && TySize == 32) ||
            (Ty.isPointer() && TySize == 32 && MemSizeInBytes == 4)) &&
           "Unsupported register bank, LLT, MemSizeInBytes combination");
    //assert(!G_SEXTLOAD && "SEXT loads not supported");
    (void)TySize;
    if (isStore)
      switch (MemSizeInBytes) {
      case 4:
        return Fgpu::SW;
      case 2:
        return Fgpu::SH;
      case 1:
        return Fgpu::SB;
      default:
        return Opc;
      }
    else
      // Unspecified extending load is selected into zeroExtending load.
      switch (MemSizeInBytes) {
      case 4:
        return Fgpu::LW;
      case 2:
        return Fgpu::LH;
      case 1:
        return Fgpu::LB;
      default:
        return Opc;
      }
  }

  if (isRegInFprb(ValueReg, MRI)) {
    if (isStore) {
      return Fgpu::SWC;
    } else {
      return Fgpu::LWC;
    }
  }

  return Opc;
}

bool FgpuInstructionSelector::buildUnalignedStore(
    MachineInstr &I, unsigned Opc, MachineOperand &BaseAddr, unsigned Offset,
    MachineMemOperand *MMO) const {
  MachineInstr *NewInst =
      BuildMI(*I.getParent(), I, I.getDebugLoc(), TII.get(Opc))
          .add(I.getOperand(0))
          .add(BaseAddr)
          .addImm(Offset)
          .addMemOperand(MMO);
  if (!constrainSelectedInstRegOperands(*NewInst, TII, TRI, RBI))
    return false;
  return true;
}

bool FgpuInstructionSelector::buildUnalignedLoad(
    MachineInstr &I, unsigned Opc, Register Dest, MachineOperand &BaseAddr,
    unsigned Offset, Register TiedDest, MachineMemOperand *MMO) const {
  MachineInstr *NewInst =
      BuildMI(*I.getParent(), I, I.getDebugLoc(), TII.get(Opc))
          .addDef(Dest)
          .add(BaseAddr)
          .addImm(Offset)
          .addUse(TiedDest)
          .addMemOperand(*I.memoperands_begin());
  if (!constrainSelectedInstRegOperands(*NewInst, TII, TRI, RBI))
    return false;
  return true;
}

bool FgpuInstructionSelector::select(MachineInstr &I) {

  MachineBasicBlock &MBB = *I.getParent();
  MachineFunction &MF = *MBB.getParent();
  MachineRegisterInfo &MRI = MF.getRegInfo();

  if (!isPreISelGenericOpcode(I.getOpcode())) {
    if (I.isCopy())
      return selectCopy(I, MRI);

    return true;
  }

  if (selectImpl(I, *CoverageInfo))
    return true;

  MachineInstr *MI = nullptr;
  using namespace TargetOpcode;

  switch (I.getOpcode()) {
  case G_PTR_ADD: {
    MI = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::ADD))
             .add(I.getOperand(0))
             .add(I.getOperand(1))
             .add(I.getOperand(2));
    break;
  }
  case G_INTTOPTR:
  case G_PTRTOINT: {
    I.setDesc(TII.get(COPY));
    return selectCopy(I, MRI);
  }
  case G_FRAME_INDEX: {
    MI = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::ADDi))
             .add(I.getOperand(0))
             .add(I.getOperand(1))
             .addImm(0);
    break;
  }
  case G_BRCOND: {
    MI = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::BNE))
             .add(I.getOperand(0))
             .addUse(Fgpu::ZERO)
             .add(I.getOperand(1));
    break;
  }
  case G_BRJT: {
    unsigned EntrySize =
        MF.getJumpTableInfo()->getEntrySize(MF.getDataLayout());
    assert(isPowerOf2_32(EntrySize) &&
           "Non-power-of-two jump-table entry size not supported.");
    return false; // TODO: support
//
//    Register JTIndex = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
//    MachineInstr *SLL = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::SLL))
//                            .addDef(JTIndex)
//                            .addUse(I.getOperand(2).getReg())
//                            .addImm(Log2_32(EntrySize));
//    if (!constrainSelectedInstRegOperands(*SLL, TII, TRI, RBI))
//      return false;
//
//    Register DestAddress = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
//    MachineInstr *ADDu = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::ADD))
//                             .addDef(DestAddress)
//                             .addUse(I.getOperand(0).getReg())
//                             .addUse(JTIndex);
//    if (!constrainSelectedInstRegOperands(*ADDu, TII, TRI, RBI))
//      return false;
//
//    Register Dest = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
//    MachineInstr *LW =
//        BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::LW))
//            .addDef(Dest)
//            .addUse(DestAddress)
//            .addJumpTableIndex(I.getOperand(1).getIndex(), FgpuII::MO_ABS_LO)
//            .addMemOperand(MF.getMachineMemOperand(
//                MachinePointerInfo(), MachineMemOperand::MOLoad, 4, Align(4)));
//    if (!constrainSelectedInstRegOperands(*LW, TII, TRI, RBI))
//      return false;
//
//    if (MF.getTarget().isPositionIndependent()) {
//      Register DestTmp = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
//      LW->getOperand(0).setReg(DestTmp);
//      MachineInstr *ADDu = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::ADD))
//                               .addDef(Dest)
//                               .addUse(DestTmp)
//                               .addUse(MF.getInfo<FgpuFunctionInfo>()
//                                           ->getGlobalBaseRegForGlobalISel(MF));
//      if (!constrainSelectedInstRegOperands(*ADDu, TII, TRI, RBI))
//        return false;
//    }
//
//    MachineInstr *Branch =
//        BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::PseudoIndirectBranch))
//            .addUse(Dest);
//    if (!constrainSelectedInstRegOperands(*Branch, TII, TRI, RBI))
//      return false;
//
//    I.eraseFromParent();
//    return true;
  }
  case G_BRINDIRECT: {
    return false; // TODO: support
//    MI = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::PseudoIndirectBranch))
//             .add(I.getOperand(0));
//    break;
  }
  case G_PHI: {
    const Register DestReg = I.getOperand(0).getReg();

    const TargetRegisterClass *DefRC = nullptr;
    if (Register::isPhysicalRegister(DestReg))
      DefRC = TRI.getRegClass(DestReg);
    else
      DefRC = getRegClassForTypeOnBank(DestReg, MRI);

    I.setDesc(TII.get(TargetOpcode::PHI));
    return RBI.constrainGenericRegister(DestReg, *DefRC, MRI);
  }
  case G_STORE:
  case G_LOAD:
  case G_ZEXTLOAD:
  case G_SEXTLOAD: {
    auto MMO = *I.memoperands_begin();
    MachineOperand BaseAddr = I.getOperand(1);
    int64_t SignedOffset = 0;
    // Try to fold load/store + G_PTR_ADD + G_CONSTANT
    // %SignedOffset:(s32) = G_CONSTANT i32 16_bit_signed_immediate
    // %Addr:(p0) = G_PTR_ADD %BaseAddr, %SignedOffset
    // %LoadResult/%StoreSrc = load/store %Addr(p0)
    // into:
    // %LoadResult/%StoreSrc = NewOpc %BaseAddr(p0), 16_bit_signed_immediate

    MachineInstr *Addr = MRI.getVRegDef(I.getOperand(1).getReg());
    if (Addr->getOpcode() == G_PTR_ADD) {
      MachineInstr *Offset = MRI.getVRegDef(Addr->getOperand(2).getReg());
      if (Offset->getOpcode() == G_CONSTANT) {
        APInt OffsetValue = Offset->getOperand(1).getCImm()->getValue();
        if (OffsetValue.isSignedIntN(16)) {
          BaseAddr = Addr->getOperand(1);
          SignedOffset = OffsetValue.getSExtValue();
        }
      }
    }

    // Unaligned memory access
    if (MMO->getAlign() < MMO->getSize() &&
        !STI.systemSupportsUnalignedAccess()) {
      if (MMO->getSize() != 4 || !isRegInGprb(I.getOperand(0).getReg(), MRI))
        return false;

      // TODO: build unaligned access
      return false;

//      if (I.getOpcode() == G_STORE) {
//        if (!buildUnalignedStore(I, Fgpu::SWL, BaseAddr, SignedOffset + 3, MMO))
//          return false;
//        if (!buildUnalignedStore(I, Fgpu::SWR, BaseAddr, SignedOffset, MMO))
//          return false;
//        I.eraseFromParent();
//        return true;
//      }
//
//      if (I.getOpcode() == G_LOAD) {
//        Register ImplDef = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
//        BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::IMPLICIT_DEF))
//            .addDef(ImplDef);
//        Register Tmp = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
//        if (!buildUnalignedLoad(I, Fgpu::LWL, Tmp, BaseAddr, SignedOffset + 3,
//                                ImplDef, MMO))
//          return false;
//        if (!buildUnalignedLoad(I, Fgpu::LWR, I.getOperand(0).getReg(),
//                                BaseAddr, SignedOffset, Tmp, MMO))
//          return false;
//        I.eraseFromParent();
//        return true;
//      }

      return false;
    }

    const unsigned NewOpc = selectLoadStoreOpCode(I, MRI);
    if (NewOpc == I.getOpcode())
      return false;

    MI = BuildMI(MBB, I, I.getDebugLoc(), TII.get(NewOpc))
             .add(I.getOperand(0))
             .add(BaseAddr)
             .addImm(SignedOffset)
             .addMemOperand(MMO);
    break;
  }
//  case G_SELECT: { //TODO: wtf is this
//    // Handle operands with pointer type.
//    MI = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::MOVN_I_I))
//             .add(I.getOperand(0))
//             .add(I.getOperand(2))
//             .add(I.getOperand(1))
//             .add(I.getOperand(3));
//    break;
//  }
  case G_IMPLICIT_DEF: {
    Register Dst = I.getOperand(0).getReg();
    MI = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::IMPLICIT_DEF))
             .addDef(Dst);

    // Set class based on register bank, there can be fpr and gpr implicit def.
    MRI.setRegClass(Dst, getRegClassForTypeOnBank(Dst, MRI));
    break;
  }
  case G_CONSTANT: {
    MachineIRBuilder B(I);
    if (!materialize32BitImm(I.getOperand(0).getReg(),
                             I.getOperand(1).getCImm()->getValue(), B))
      return false;

    I.eraseFromParent();
    return true;
  }
  case G_GLOBAL_VALUE: {
    const llvm::GlobalValue *GVal = I.getOperand(1).getGlobal();
    if (MF.getTarget().isPositionIndependent()) {
      MachineInstr *LWGOT = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::LW))
                                .addDef(I.getOperand(0).getReg())
                                .addReg(MF.getInfo<FgpuFunctionInfo>()
                                            ->getGlobalBaseRegForGlobalISel(MF))
                                .addGlobalAddress(GVal);
      // Global Values that don't have local linkage are handled differently
      // when they are part of call sequence. FgpuCallLowering::lowerCall
      // creates G_GLOBAL_VALUE instruction as part of call sequence and adds
      // MO_GOT_CALL flag when Callee doesn't have local linkage.
      if (I.getOperand(1).getTargetFlags() == FgpuII::MO_GOT_CALL)
        LWGOT->getOperand(2).setTargetFlags(FgpuII::MO_GOT_CALL);
      else
        LWGOT->getOperand(2).setTargetFlags(FgpuII::MO_GOT);
      LWGOT->addMemOperand(
          MF, MF.getMachineMemOperand(MachinePointerInfo::getGOT(MF),
                                      MachineMemOperand::MOLoad, 4, Align(4)));
      if (!constrainSelectedInstRegOperands(*LWGOT, TII, TRI, RBI))
        return false;

      if (GVal->hasLocalLinkage()) {
        Register LWGOTDef = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
        LWGOT->getOperand(0).setReg(LWGOTDef);

        MachineInstr *ADDiu =
            BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::ADDi))
                .addDef(I.getOperand(0).getReg())
                .addReg(LWGOTDef)
                .addGlobalAddress(GVal);
        ADDiu->getOperand(2).setTargetFlags(FgpuII::MO_ABS_LO);
        if (!constrainSelectedInstRegOperands(*ADDiu, TII, TRI, RBI))
          return false;
      }
    } else {
      Register LUiReg = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);

      MachineInstr *LUi = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::LUi))
                              .addDef(LUiReg)
                              .addGlobalAddress(GVal);
      LUi->getOperand(1).setTargetFlags(FgpuII::MO_ABS_HI);
      if (!constrainSelectedInstRegOperands(*LUi, TII, TRI, RBI))
        return false;

      MachineInstr *ADDiu = // TODO: shoudl this be LI??
          BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::ADDi))
              .addDef(I.getOperand(0).getReg())
              .addUse(LUiReg)
              .addGlobalAddress(GVal);
      ADDiu->getOperand(2).setTargetFlags(FgpuII::MO_ABS_LO);
      if (!constrainSelectedInstRegOperands(*ADDiu, TII, TRI, RBI))
        return false;
    }
    I.eraseFromParent();
    return true;
  }
  case G_JUMP_TABLE: {
    if (MF.getTarget().isPositionIndependent()) {
      MI = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::LW))
               .addDef(I.getOperand(0).getReg())
               .addReg(MF.getInfo<FgpuFunctionInfo>()
                           ->getGlobalBaseRegForGlobalISel(MF))
               .addJumpTableIndex(I.getOperand(1).getIndex(), FgpuII::MO_GOT)
               .addMemOperand(MF.getMachineMemOperand(
                   MachinePointerInfo::getGOT(MF), MachineMemOperand::MOLoad, 4,
                   Align(4)));
    } else {
      MI =
          BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::LUi))
              .addDef(I.getOperand(0).getReg())
              .addJumpTableIndex(I.getOperand(1).getIndex(), FgpuII::MO_ABS_HI);
    }
    break;
  }
  case G_ICMP: {
    struct Instr {
      unsigned Opcode;
      Register Def, LHS, RHS;
      Instr(unsigned Opcode, Register Def, Register LHS, Register RHS)
          : Opcode(Opcode), Def(Def), LHS(LHS), RHS(RHS){};

      bool hasImm() const {
        if (Opcode == Fgpu::SLTiu || Opcode == Fgpu::XORi)
          return true;
        return false;
      }
    };

    SmallVector<struct Instr, 2> Instructions;
    Register ICMPReg = I.getOperand(0).getReg();
    Register Temp = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
    Register LHS = I.getOperand(2).getReg();
    Register RHS = I.getOperand(3).getReg();
    CmpInst::Predicate Cond =
        static_cast<CmpInst::Predicate>(I.getOperand(1).getPredicate());

    switch (Cond) {
    case CmpInst::ICMP_EQ: // LHS == RHS -> (LHS ^ RHS) < 1
      Instructions.emplace_back(Fgpu::XOR, Temp, LHS, RHS);
      Instructions.emplace_back(Fgpu::SLTiu, ICMPReg, Temp, 1);
      break;
    case CmpInst::ICMP_NE: // LHS != RHS -> 0 < (LHS ^ RHS)
      Instructions.emplace_back(Fgpu::XOR, Temp, LHS, RHS);
      Instructions.emplace_back(Fgpu::SLTu, ICMPReg, Fgpu::ZERO, Temp);
      break;
    case CmpInst::ICMP_UGT: // LHS >  RHS -> RHS < LHS
      Instructions.emplace_back(Fgpu::SLTu, ICMPReg, RHS, LHS);
      break;
    case CmpInst::ICMP_UGE: // LHS >= RHS -> !(LHS < RHS)
      Instructions.emplace_back(Fgpu::SLTu, Temp, LHS, RHS);
      Instructions.emplace_back(Fgpu::XORi, ICMPReg, Temp, 1);
      break;
    case CmpInst::ICMP_ULT: // LHS <  RHS -> LHS < RHS
      Instructions.emplace_back(Fgpu::SLTu, ICMPReg, LHS, RHS);
      break;
    case CmpInst::ICMP_ULE: // LHS <= RHS -> !(RHS < LHS)
      Instructions.emplace_back(Fgpu::SLTu, Temp, RHS, LHS);
      Instructions.emplace_back(Fgpu::XORi, ICMPReg, Temp, 1);
      break;
    case CmpInst::ICMP_SGT: // LHS >  RHS -> RHS < LHS
      Instructions.emplace_back(Fgpu::SLT, ICMPReg, RHS, LHS);
      break;
    case CmpInst::ICMP_SGE: // LHS >= RHS -> !(LHS < RHS)
      Instructions.emplace_back(Fgpu::SLT, Temp, LHS, RHS);
      Instructions.emplace_back(Fgpu::XORi, ICMPReg, Temp, 1);
      break;
    case CmpInst::ICMP_SLT: // LHS <  RHS -> LHS < RHS
      Instructions.emplace_back(Fgpu::SLT, ICMPReg, LHS, RHS);
      break;
    case CmpInst::ICMP_SLE: // LHS <= RHS -> !(RHS < LHS)
      Instructions.emplace_back(Fgpu::SLT, Temp, RHS, LHS);
      Instructions.emplace_back(Fgpu::XORi, ICMPReg, Temp, 1);
      break;
    default:
      return false;
    }

    MachineIRBuilder B(I);
    for (const struct Instr &Instruction : Instructions) {
      MachineInstrBuilder MIB = B.buildInstr(
          Instruction.Opcode, {Instruction.Def}, {Instruction.LHS});

      if (Instruction.hasImm())
        MIB.addImm(Instruction.RHS);
      else
        MIB.addUse(Instruction.RHS);

      if (!MIB.constrainAllUses(TII, TRI, RBI))
        return false;
    }

    I.eraseFromParent();
    return true;
  }
//  case G_FCMP: {
//    unsigned FgpuFCMPCondCode;
//    bool isLogicallyNegated;
//    switch (CmpInst::Predicate Cond = static_cast<CmpInst::Predicate>(
//                I.getOperand(1).getPredicate())) {
//    case CmpInst::FCMP_UNO: // Unordered
//    case CmpInst::FCMP_ORD: // Ordered (OR)
//      FgpuFCMPCondCode = Fgpu::FCOND_UN;
//      isLogicallyNegated = Cond != CmpInst::FCMP_UNO;
//      break;
//    case CmpInst::FCMP_OEQ: // Equal
//    case CmpInst::FCMP_UNE: // Not Equal (NEQ)
//      FgpuFCMPCondCode = Fgpu::FCOND_OEQ;
//      isLogicallyNegated = Cond != CmpInst::FCMP_OEQ;
//      break;
//    case CmpInst::FCMP_UEQ: // Unordered or Equal
//    case CmpInst::FCMP_ONE: // Ordered or Greater Than or Less Than (OGL)
//      FgpuFCMPCondCode = Fgpu::FCOND_UEQ;
//      isLogicallyNegated = Cond != CmpInst::FCMP_UEQ;
//      break;
//    case CmpInst::FCMP_OLT: // Ordered or Less Than
//    case CmpInst::FCMP_UGE: // Unordered or Greater Than or Equal (UGE)
//      FgpuFCMPCondCode = Fgpu::FCOND_OLT;
//      isLogicallyNegated = Cond != CmpInst::FCMP_OLT;
//      break;
//    case CmpInst::FCMP_ULT: // Unordered or Less Than
//    case CmpInst::FCMP_OGE: // Ordered or Greater Than or Equal (OGE)
//      FgpuFCMPCondCode = Fgpu::FCOND_ULT;
//      isLogicallyNegated = Cond != CmpInst::FCMP_ULT;
//      break;
//    case CmpInst::FCMP_OLE: // Ordered or Less Than or Equal
//    case CmpInst::FCMP_UGT: // Unordered or Greater Than (UGT)
//      FgpuFCMPCondCode = Fgpu::FCOND_OLE;
//      isLogicallyNegated = Cond != CmpInst::FCMP_OLE;
//      break;
//    case CmpInst::FCMP_ULE: // Unordered or Less Than or Equal
//    case CmpInst::FCMP_OGT: // Ordered or Greater Than (OGT)
//      FgpuFCMPCondCode = Fgpu::FCOND_ULE;
//      isLogicallyNegated = Cond != CmpInst::FCMP_ULE;
//      break;
//    default:
//      return false;
//    }
//
//    // Default compare result in gpr register will be `true`.
//    // We will move `false` (FGPU::Zero) to gpr result when fcmp gives false
//    // using MOVF_I. When orignal predicate (Cond) is logically negated
//    // FgpuFCMPCondCode, result is inverted i.e. MOVT_I is used.
//    unsigned MoveOpcode = isLogicallyNegated ? Fgpu::MOVT_I : Fgpu::MOVF_I;
//
//    Register TrueInReg = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
//    BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::ADDi))
//        .addDef(TrueInReg)
//        .addUse(Fgpu::ZERO)
//        .addImm(1);
//
//    unsigned Size = MRI.getType(I.getOperand(2).getReg()).getSizeInBits();
//    unsigned FCMPOpcode =
//        Size == 32 ? Fgpu::FCMP_S32
//                   : STI.isFP64bit() ? Fgpu::FCMP_D64 : Fgpu::FCMP_D32;
//    MachineInstr *FCMP = BuildMI(MBB, I, I.getDebugLoc(), TII.get(FCMPOpcode))
//                             .addUse(I.getOperand(2).getReg())
//                             .addUse(I.getOperand(3).getReg())
//                             .addImm(FgpuFCMPCondCode);
//    if (!constrainSelectedInstRegOperands(*FCMP, TII, TRI, RBI))
//      return false;
//
//    MachineInstr *Move = BuildMI(MBB, I, I.getDebugLoc(), TII.get(MoveOpcode))
//                             .addDef(I.getOperand(0).getReg())
//                             .addUse(Fgpu::ZERO)
//                             .addUse(Fgpu::FCC0)
//                             .addUse(TrueInReg);
//    if (!constrainSelectedInstRegOperands(*Move, TII, TRI, RBI))
//      return false;
//
//    I.eraseFromParent();
//    return true;
//  }
//  case G_FENCE: {
//    MI = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::SYNC)).addImm(0);
//    break;
//  }
  case G_VASTART: {
    FgpuFunctionInfo *FuncInfo = MF.getInfo<FgpuFunctionInfo>();
    int FI = FuncInfo->getVarArgsFrameIndex();

    Register LeaReg = MRI.createVirtualRegister(&Fgpu::GPROutRegClass);
    MachineInstr *LEA_ADDiu =
        BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::LEA_ADDiu))
            .addDef(LeaReg)
            .addFrameIndex(FI)
            .addImm(0);
    if (!constrainSelectedInstRegOperands(*LEA_ADDiu, TII, TRI, RBI))
      return false;

    MachineInstr *Store = BuildMI(MBB, I, I.getDebugLoc(), TII.get(Fgpu::SW))
                              .addUse(LeaReg)
                              .addUse(I.getOperand(0).getReg())
                              .addImm(0);
    if (!constrainSelectedInstRegOperands(*Store, TII, TRI, RBI))
      return false;

    I.eraseFromParent();
    return true;
  }
  default:
    return false;
  }

  I.eraseFromParent();
  return constrainSelectedInstRegOperands(*MI, TII, TRI, RBI);
}

namespace llvm {
InstructionSelector *createFgpuInstructionSelector(const FgpuTargetMachine &TM,
                                                   FgpuSubtarget &Subtarget,
                                                   FgpuRegisterBankInfo &RBI) {
  return new FgpuInstructionSelector(TM, Subtarget, RBI);
}
} // end namespace llvm
